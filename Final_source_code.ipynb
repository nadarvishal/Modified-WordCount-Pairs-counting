{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5c8da09c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Madhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contentfile-20090224.html.txt  word count is: 1359\n",
      "contentfile-20100127.html.txt  word count is: 1551\n",
      "contentfile-20110125.html.txt  word count is: 1544\n",
      "contentfile-20120124.html.txt  word count is: 1599\n",
      "The average of each word is as follows\n",
      "The standard deviation of each word is as follows\n",
      "20130212.html\n",
      "the next year following the window is as follows\n",
      "contentfile-20130212.html.txt\n",
      "the frequency word exceeding the average plus two standard deviations is :- absolutely\n",
      "the frequency word exceeding the average plus two standard deviations is :- accept\n",
      "the frequency word exceeding the average plus two standard deviations is :- achieve\n",
      "the frequency word exceeding the average plus two standard deviations is :- added\n",
      "the frequency word exceeding the average plus two standard deviations is :- affiliates\n",
      "the frequency word exceeding the average plus two standard deviations is :- agenda\n",
      "the frequency word exceeding the average plus two standard deviations is :- ago\n",
      "the frequency word exceeding the average plus two standard deviations is :- agree\n",
      "the frequency word exceeding the average plus two standard deviations is :- ahead\n",
      "the frequency word exceeding the average plus two standard deviations is :- air\n",
      "the frequency word exceeding the average plus two standard deviations is :- allies\n",
      "the frequency word exceeding the average plus two standard deviations is :- along\n",
      "the frequency word exceeding the average plus two standard deviations is :- americas\n",
      "the frequency word exceeding the average plus two standard deviations is :- announce\n",
      "the frequency word exceeding the average plus two standard deviations is :- arabian\n",
      "the frequency word exceeding the average plus two standard deviations is :- aside\n",
      "the frequency word exceeding the average plus two standard deviations is :- ask\n",
      "the frequency word exceeding the average plus two standard deviations is :- asking\n",
      "the frequency word exceeding the average plus two standard deviations is :- attacks\n",
      "the frequency word exceeding the average plus two standard deviations is :- available\n",
      "the frequency word exceeding the average plus two standard deviations is :- based\n",
      "the frequency word exceeding the average plus two standard deviations is :- batteries\n",
      "the frequency word exceeding the average plus two standard deviations is :- bipartisan\n",
      "the frequency word exceeding the average plus two standard deviations is :- border\n",
      "the frequency word exceeding the average plus two standard deviations is :- bridges\n",
      "the frequency word exceeding the average plus two standard deviations is :- buildings\n",
      "the frequency word exceeding the average plus two standard deviations is :- campaign\n",
      "the frequency word exceeding the average plus two standard deviations is :- cars\n",
      "the frequency word exceeding the average plus two standard deviations is :- ceo\n",
      "the frequency word exceeding the average plus two standard deviations is :- choose\n",
      "the frequency word exceeding the average plus two standard deviations is :- citizens\n",
      "the frequency word exceeding the average plus two standard deviations is :- collapse\n",
      "the frequency word exceeding the average plus two standard deviations is :- combat\n",
      "the frequency word exceeding the average plus two standard deviations is :- communities\n",
      "the frequency word exceeding the average plus two standard deviations is :- complicated\n",
      "the frequency word exceeding the average plus two standard deviations is :- congress\n",
      "the frequency word exceeding the average plus two standard deviations is :- connected\n",
      "the frequency word exceeding the average plus two standard deviations is :- convinced\n",
      "the frequency word exceeding the average plus two standard deviations is :- core\n",
      "the frequency word exceeding the average plus two standard deviations is :- countries\n",
      "the frequency word exceeding the average plus two standard deviations is :- courage\n",
      "the frequency word exceeding the average plus two standard deviations is :- creation\n",
      "the frequency word exceeding the average plus two standard deviations is :- cyber\n",
      "the frequency word exceeding the average plus two standard deviations is :- decisions\n",
      "the frequency word exceeding the average plus two standard deviations is :- deeper\n",
      "the frequency word exceeding the average plus two standard deviations is :- democracy\n",
      "the frequency word exceeding the average plus two standard deviations is :- deserve\n",
      "the frequency word exceeding the average plus two standard deviations is :- developing\n",
      "the frequency word exceeding the average plus two standard deviations is :- diplomatic\n",
      "the frequency word exceeding the average plus two standard deviations is :- drive\n",
      "the frequency word exceeding the average plus two standard deviations is :- earlier\n",
      "the frequency word exceeding the average plus two standard deviations is :- earned\n",
      "the frequency word exceeding the average plus two standard deviations is :- east\n",
      "the frequency word exceeding the average plus two standard deviations is :- economists\n",
      "the frequency word exceeding the average plus two standard deviations is :- efforts\n",
      "the frequency word exceeding the average plus two standard deviations is :- elected\n",
      "the frequency word exceeding the average plus two standard deviations is :- enforcement\n",
      "the frequency word exceeding the average plus two standard deviations is :- engine\n",
      "the frequency word exceeding the average plus two standard deviations is :- engineering\n",
      "the frequency word exceeding the average plus two standard deviations is :- ensures\n",
      "the frequency word exceeding the average plus two standard deviations is :- entire\n",
      "the frequency word exceeding the average plus two standard deviations is :- europe\n",
      "the frequency word exceeding the average plus two standard deviations is :- example\n",
      "the frequency word exceeding the average plus two standard deviations is :- expand\n",
      "the frequency word exceeding the average plus two standard deviations is :- expanding\n",
      "the frequency word exceeding the average plus two standard deviations is :- expect\n",
      "the frequency word exceeding the average plus two standard deviations is :- faith\n",
      "the frequency word exceeding the average plus two standard deviations is :- fallen\n",
      "the frequency word exceeding the average plus two standard deviations is :- fellow\n",
      "the frequency word exceeding the average plus two standard deviations is :- field\n",
      "the frequency word exceeding the average plus two standard deviations is :- fifteen\n",
      "the frequency word exceeding the average plus two standard deviations is :- firefighters\n",
      "the frequency word exceeding the average plus two standard deviations is :- fix\n",
      "the frequency word exceeding the average plus two standard deviations is :- follow\n",
      "the frequency word exceeding the average plus two standard deviations is :- forces\n",
      "the frequency word exceeding the average plus two standard deviations is :- free\n",
      "the frequency word exceeding the average plus two standard deviations is :- freedom\n",
      "the frequency word exceeding the average plus two standard deviations is :- friend\n",
      "the frequency word exceeding the average plus two standard deviations is :- full\n",
      "the frequency word exceeding the average plus two standard deviations is :- fully\n",
      "the frequency word exceeding the average plus two standard deviations is :- gay\n",
      "the frequency word exceeding the average plus two standard deviations is :- generations\n",
      "the frequency word exceeding the average plus two standard deviations is :- germany\n",
      "the frequency word exceeding the average plus two standard deviations is :- girl\n",
      "the frequency word exceeding the average plus two standard deviations is :- god\n",
      "the frequency word exceeding the average plus two standard deviations is :- grants\n",
      "the frequency word exceeding the average plus two standard deviations is :- grow\n",
      "the frequency word exceeding the average plus two standard deviations is :- growth\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-28f71b4d93a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    170\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0my2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[0mt1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m         \u001b[0mt1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mavg_sd_rdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlookup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m<\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"the frequency word exceeding the average plus two standard deviations is :-\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\BigData\\Spark\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mlookup\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2607\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2608\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2609\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2610\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2611\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_to_java_object_rdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\BigData\\Spark\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    947\u001b[0m         \"\"\"\n\u001b[0;32m    948\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 949\u001b[1;33m             \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    950\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\BigData\\Spark\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1301\u001b[0m             \u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1303\u001b[1;33m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[1;32mC:\\BigData\\Spark\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1031\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1032\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1033\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1034\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1035\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\BigData\\Spark\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m   1198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1199\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1200\u001b[1;33m             \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1201\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Answer received: {0}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1202\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    667\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 669\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    670\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#In this code we can get the word-count of data starting from year 2009.\n",
    "#we also get the average and standard deviation of words in the interval of 2009-2012, 2013- 2016 and 2017-2020\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from statistics import stdev\n",
    "import re\n",
    "import urllib.request\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords =stopwords.words('english')\n",
    "\n",
    "#here we are creating a new spark session\n",
    "spark = SparkSession.builder\\\n",
    "                    .master(\"local\")\\\n",
    "                    .appName('FirstAssignment1part1')\\\n",
    "                    .getOrCreate()\n",
    "sc=spark.sparkContext\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36',\n",
    "}\n",
    "\n",
    "#loading the website and its contents\n",
    "url = \"http://stateoftheunion.onetwothree.net/texts/index.html\"\n",
    "requ = urllib.request.Request(url,headers=headers)\n",
    "req_url = urllib.request.urlopen(requ) \n",
    "contents = req_url.read()\n",
    "contents = str(contents)\n",
    "\n",
    "#writing the contents into alldata file.\n",
    "f = open('alldata.txt', 'w')\n",
    "f.writelines(contents)\n",
    "f.close()\n",
    "f = open('alldata.txt','r')\n",
    "contents = f.read()\n",
    "h_r = r'href=[\\'\"]?([^\\'\" >]+)'\n",
    "pagenames = re.findall(h_r, contents)\n",
    "allnames = list()\n",
    "\n",
    "#creating and saving the contents of each link into thier specific txt file such as contentfile-17901208.html.txt\n",
    "for index,pagename in enumerate(pagenames):\n",
    "    if pagename[0:2] == '17' or pagename[0:2] == '18' or pagename[0:2] == '19' or pagename[0:2] == '20':\n",
    "        allnames.append(pagename)\n",
    "        file = open('contentfile-%s.txt' % pagename, 'w')\n",
    "        url = \"http://stateoftheunion.onetwothree.net/texts/\"+pagename\n",
    "        req = urllib.request.Request(url,headers=headers)\n",
    "        request_url = urllib.request.urlopen(req) \n",
    "        contents = request_url.read()\n",
    "        file.write(str(contents))      \n",
    "\n",
    "#here we are removing the punctuation, html, or stopwords and removing them\n",
    "def RemoveNT(a):\n",
    "    a = a.replace(\"\\\\n\", \" \")\n",
    "    a = a.replace(\"\\\\t\", \" \")\n",
    "    return a\n",
    "\n",
    "def RemovePunc(a):\n",
    "    punc='!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~-'\n",
    "    lowcase = a.lower()\n",
    "    for ch in punc:\n",
    "        lowcase = lowcase.replace(ch, \" \")\n",
    "    return lowcase\n",
    "\n",
    "html_words = [\"href\",\"div\",\"index\",\"style\",\"tabs\",\"cell\",\"html\",\"link\",\n",
    "              \"src\",\"script\",\"image\",\"center\",\"alt\",\"html\",\"xml\",\"version\",\"meta\"\n",
    "              \"css\",\"http\",\"type\",\"script\",\"class\",\"text\",\"instance\"]\n",
    "def RemoveHtml(a):\n",
    "    for word in html_words:\n",
    "        if a.count(word) > 0:\n",
    "            a = \" \"\n",
    "    return a\n",
    "        \n",
    "Tags = re.compile(r'<[^>]+>')\n",
    "def RemoveTags(a):\n",
    "    return Tags.sub(\"\", a)\n",
    "\n",
    "def RemoveStopw(a):\n",
    "    return \" \".join([word for word in str(a).split() if word not in stopwords])\n",
    "\n",
    "def RemoveDigit(a):\n",
    "    res = any(chr.isdigit() for chr in a)\n",
    "    if res:\n",
    "        a = \" \"\n",
    "    return a\n",
    "\n",
    "my_rdd = sc.emptyRDD()\n",
    "my_rdd_list = list()\n",
    "count = 0\n",
    "a=0\n",
    "alllinksnew = allnames[222:] #this starts the data to come starting from 2009\n",
    "for i in range(3):\n",
    "    alllinktemp = alllinksnew[a:4+a]#so seperate data into years 2009-2012, 2013- 2016 and 2017-2020\n",
    "    for index,pagename in enumerate(alllinktemp):\n",
    "        filename = \"contentfile-\"+pagename+\".txt\"\n",
    "        my_rdd=sc.textFile(filename)\n",
    "        my_rdd=my_rdd.flatMap(lambda satir: satir.split(\" \"))\n",
    "        my_rdd = my_rdd.map(RemoveNT)\n",
    "        my_rdd = my_rdd.map(RemovePunc)\n",
    "        my_rdd = my_rdd.map(RemoveHtml)\n",
    "        my_rdd = my_rdd.map(RemoveTags)\n",
    "        my_rdd = my_rdd.map(RemoveStopw)\n",
    "        my_rdd = my_rdd.map(RemoveDigit)\n",
    "        my_rdd=my_rdd.flatMap(lambda satir: satir.split(\" \"))\n",
    "        my_rdd = my_rdd.filter(lambda x:x!='')\n",
    "        wordCount=my_rdd.map(lambda  word:(word,1))\n",
    "        wordCount = wordCount.reduceByKey(lambda x,y:(x+y)).sortByKey()\n",
    "        print(filename,\" word count is:\",wordCount.count())\n",
    "        my_rdd_list.append(wordCount)\n",
    "    rdd=0\n",
    "    rdd2=0\n",
    "    rdd = sc.union(my_rdd_list)\n",
    "    \n",
    "    rdd2 = rdd.reduceByKey(lambda x,y:(x+y)).sortByKey()\n",
    "    #for average we have divided by 4 as there are 4 year groups\n",
    "    rdd2 = rdd2.mapValues(lambda x: x / 4)\n",
    "    rdd2 = rdd2.sortBy(lambda x:x[1],ascending=False)\n",
    "    #here we are print the average of each word\n",
    "    print(\"The average of each word is as follows\")\n",
    "    print(rdd2.take(10))\n",
    "  \n",
    "    \n",
    "    y = rdd.groupByKey()\n",
    "    y = y.sortByKey()\n",
    "    # this creates a calculation of standard deviation with result as false for those whose len is less than 1 \n",
    "    y1 = y.mapValues(lambda x: len(x)>1 and stdev(x))\n",
    "    y1 = y1.sortBy(lambda x:x[1],ascending=False)\n",
    "    print(\"The standard deviation of each word is as follows\")\n",
    "    print(y1.take(10))\n",
    "    \n",
    "    avg_sd_list = list()\n",
    "    y_prev=y\n",
    "    for t in y.collect():\n",
    "        if len(t[1]) > 1:\n",
    "            #here we are storing the average plus twice the standard deviation \n",
    "            avg_sd_list.append((t[0],(sum(t[1])/4) +(2 * stdev(t[1]))))\n",
    "    avg_sd_rdd = sc.parallelize(avg_sd_list)\n",
    "    #here we print the average and twice standard deviation of each word if needed\n",
    "    #print(\"The average of twice standard deviation of each word is as follows\")\n",
    "    #print(avg_sd_rdd.collect())\n",
    "    \n",
    "    #here we compute the data for the year following the window such as year 2013 for window 2009-2012\n",
    "    my_rdd2 = sc.emptyRDD()\n",
    "    my_rdd2_list = list()\n",
    "    alllinktemp2 = alllinksnew[4+a]\n",
    "    print(alllinktemp2)\n",
    "    index=0\n",
    "    pagename=0\n",
    "    filename1 = \"contentfile-\"+ alllinktemp2 +\".txt\"\n",
    "    print(\"the next year following the window is as follows\")\n",
    "    print(filename1)\n",
    "    my_rdd2 = sc.textFile(filename1)\n",
    "    my_rdd2 = my_rdd2.flatMap(lambda satir: satir.split(\" \"))\n",
    "    my_rdd2 = my_rdd2.map(RemoveNT)\n",
    "    my_rdd2 = my_rdd2.map(RemovePunc)\n",
    "    my_rdd2 = my_rdd2.map(RemoveHtml)\n",
    "    my_rdd2 = my_rdd2.map(RemoveTags)\n",
    "    my_rdd2 = my_rdd2.map(RemoveStopw)\n",
    "    my_rdd2 = my_rdd2.map(RemoveDigit)\n",
    "    my_rdd2 = my_rdd2.flatMap(lambda satir: satir.split(\" \"))\n",
    "    my_rdd2 = my_rdd2.filter(lambda x:x!='')\n",
    "    word_count2 = my_rdd2.map(lambda  word:(word,1))\n",
    "    word_count2 = word_count2.reduceByKey(lambda x,y:(x+y)).sortByKey()\n",
    "    #print(word_count2.count())\n",
    "    my_rdd2_list.append(word_count2) \n",
    "    rdd2=0\n",
    "    rdd2 = sc.union(my_rdd2_list)\n",
    "    rdd2.collect()\n",
    "    y2 = rdd2.groupByKey()\n",
    "    for t in y2.collect():\n",
    "        t1=0\n",
    "        t1=avg_sd_rdd.lookup(t[0])\n",
    "        if(sum(t1)<sum(t[1]) and sum(t1)!=0):\n",
    "            print(\"the frequency word exceeding the average plus two standard deviations is :-\",t[0])\n",
    "    a+=4\n",
    "\n",
    "# To stop the current spark session\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9af3adbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Madhu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contentfile-17900108.html.txt\n",
      "contentfile-17901208.html.txt\n",
      "contentfile-17911025.html.txt\n",
      "contentfile-17921106.html.txt\n",
      "contentfile-17931203.html.txt\n",
      "contentfile-17941119.html.txt\n",
      "contentfile-17951208.html.txt\n",
      "contentfile-17961207.html.txt\n",
      "contentfile-17971122.html.txt\n",
      "contentfile-17981208.html.txt\n",
      "contentfile-17991203.html.txt\n",
      "contentfile-18001111.html.txt\n",
      "contentfile-18011208.html.txt\n",
      "contentfile-18021215.html.txt\n",
      "contentfile-18031017.html.txt\n",
      "contentfile-18041108.html.txt\n",
      "contentfile-18051203.html.txt\n",
      "contentfile-18061202.html.txt\n",
      "contentfile-18071027.html.txt\n",
      "contentfile-18081108.html.txt\n",
      "contentfile-18091129.html.txt\n",
      "contentfile-18101205.html.txt\n",
      "contentfile-18111105.html.txt\n",
      "contentfile-18121104.html.txt\n",
      "contentfile-18131207.html.txt\n",
      "contentfile-18140920.html.txt\n",
      "contentfile-18151205.html.txt\n",
      "contentfile-18161203.html.txt\n",
      "contentfile-18171212.html.txt\n",
      "contentfile-18181116.html.txt\n",
      "contentfile-18191207.html.txt\n",
      "contentfile-18201114.html.txt\n",
      "contentfile-18211203.html.txt\n",
      "contentfile-18221203.html.txt\n",
      "contentfile-18231202.html.txt\n",
      "contentfile-18241207.html.txt\n",
      "contentfile-18251206.html.txt\n",
      "contentfile-18261205.html.txt\n",
      "contentfile-18271204.html.txt\n",
      "contentfile-18281202.html.txt\n",
      "contentfile-18291208.html.txt\n",
      "contentfile-18301206.html.txt\n",
      "contentfile-18311206.html.txt\n",
      "contentfile-18321204.html.txt\n",
      "contentfile-18331203.html.txt\n",
      "contentfile-18341201.html.txt\n",
      "contentfile-18351207.html.txt\n",
      "contentfile-18361205.html.txt\n",
      "contentfile-18371205.html.txt\n",
      "contentfile-18381203.html.txt\n",
      "contentfile-18391202.html.txt\n",
      "contentfile-18401205.html.txt\n",
      "contentfile-18411207.html.txt\n",
      "contentfile-18421206.html.txt\n",
      "contentfile-18431206.html.txt\n",
      "contentfile-18441203.html.txt\n",
      "contentfile-18451202.html.txt\n",
      "contentfile-18461208.html.txt\n",
      "contentfile-18471207.html.txt\n",
      "contentfile-18481205.html.txt\n",
      "contentfile-18491204.html.txt\n",
      "contentfile-18501202.html.txt\n",
      "contentfile-18511202.html.txt\n",
      "contentfile-18521206.html.txt\n",
      "contentfile-18531205.html.txt\n",
      "contentfile-18541204.html.txt\n",
      "contentfile-18551231.html.txt\n",
      "contentfile-18561202.html.txt\n",
      "contentfile-18571208.html.txt\n",
      "contentfile-18581206.html.txt\n",
      "contentfile-18591219.html.txt\n",
      "contentfile-18601203.html.txt\n",
      "contentfile-18611203.html.txt\n",
      "contentfile-18621201.html.txt\n",
      "contentfile-18631208.html.txt\n",
      "contentfile-18641206.html.txt\n",
      "contentfile-18651204.html.txt\n",
      "contentfile-18661203.html.txt\n",
      "contentfile-18671203.html.txt\n",
      "contentfile-18681209.html.txt\n",
      "contentfile-18691206.html.txt\n",
      "contentfile-18701205.html.txt\n",
      "contentfile-18711204.html.txt\n",
      "contentfile-18721202.html.txt\n",
      "contentfile-18731201.html.txt\n",
      "contentfile-18741207.html.txt\n",
      "contentfile-18751207.html.txt\n",
      "contentfile-18761205.html.txt\n",
      "contentfile-18771203.html.txt\n",
      "contentfile-18781202.html.txt\n",
      "contentfile-18791201.html.txt\n",
      "contentfile-18801206.html.txt\n",
      "contentfile-18811206.html.txt\n",
      "contentfile-18821204.html.txt\n",
      "contentfile-18831204.html.txt\n",
      "contentfile-18841201.html.txt\n",
      "contentfile-18851208.html.txt\n",
      "contentfile-18861206.html.txt\n",
      "contentfile-18871206.html.txt\n",
      "contentfile-18881203.html.txt\n",
      "contentfile-18891203.html.txt\n",
      "contentfile-18901201.html.txt\n",
      "contentfile-18911209.html.txt\n",
      "contentfile-18921206.html.txt\n",
      "contentfile-18931203.html.txt\n",
      "contentfile-18941202.html.txt\n",
      "contentfile-18951207.html.txt\n",
      "contentfile-18961204.html.txt\n",
      "contentfile-18971206.html.txt\n",
      "contentfile-18981205.html.txt\n",
      "contentfile-18991205.html.txt\n",
      "contentfile-19001203.html.txt\n",
      "contentfile-19011203.html.txt\n",
      "contentfile-19021202.html.txt\n",
      "contentfile-19031207.html.txt\n",
      "contentfile-19041206.html.txt\n",
      "contentfile-19051205.html.txt\n",
      "contentfile-19061203.html.txt\n",
      "contentfile-19071203.html.txt\n",
      "contentfile-19081208.html.txt\n",
      "contentfile-19091207.html.txt\n",
      "contentfile-19101206.html.txt\n",
      "contentfile-19111205.html.txt\n",
      "contentfile-19121203.html.txt\n",
      "contentfile-19131202.html.txt\n",
      "contentfile-19141208.html.txt\n",
      "contentfile-19151207.html.txt\n",
      "contentfile-19161205.html.txt\n",
      "contentfile-19171204.html.txt\n",
      "contentfile-19181202.html.txt\n",
      "contentfile-19191202.html.txt\n",
      "contentfile-19201207.html.txt\n",
      "contentfile-19211206.html.txt\n",
      "contentfile-19221208.html.txt\n",
      "contentfile-19231206.html.txt\n",
      "contentfile-19241203.html.txt\n",
      "contentfile-19251208.html.txt\n",
      "contentfile-19261207.html.txt\n",
      "contentfile-19271206.html.txt\n",
      "contentfile-19281204.html.txt\n",
      "contentfile-19291203.html.txt\n",
      "contentfile-19301202.html.txt\n",
      "contentfile-19311208.html.txt\n",
      "contentfile-19321206.html.txt\n",
      "contentfile-19340103.html.txt\n",
      "contentfile-19350104.html.txt\n",
      "contentfile-19360103.html.txt\n",
      "contentfile-19370106.html.txt\n",
      "contentfile-19380103.html.txt\n",
      "contentfile-19390104.html.txt\n",
      "contentfile-19400103.html.txt\n",
      "contentfile-19410106.html.txt\n",
      "contentfile-19420106.html.txt\n",
      "contentfile-19430107.html.txt\n",
      "contentfile-19440111.html.txt\n",
      "contentfile-19450106.html.txt\n",
      "contentfile-19460121.html.txt\n",
      "contentfile-19470106.html.txt\n",
      "contentfile-19480107.html.txt\n",
      "contentfile-19490105.html.txt\n",
      "contentfile-19500104.html.txt\n",
      "contentfile-19510108.html.txt\n",
      "contentfile-19520109.html.txt\n",
      "contentfile-19530107.html.txt\n",
      "contentfile-19530202.html.txt\n",
      "contentfile-19540107.html.txt\n",
      "contentfile-19550106.html.txt\n",
      "contentfile-19560105.html.txt\n",
      "contentfile-19570110.html.txt\n",
      "contentfile-19580109.html.txt\n",
      "contentfile-19590109.html.txt\n",
      "contentfile-19600107.html.txt\n",
      "contentfile-19610112.html.txt\n",
      "contentfile-19610130.html.txt\n",
      "contentfile-19620111.html.txt\n",
      "contentfile-19630114.html.txt\n",
      "contentfile-19640108.html.txt\n",
      "contentfile-19650104.html.txt\n",
      "contentfile-19660112.html.txt\n",
      "contentfile-19670110.html.txt\n",
      "contentfile-19680117.html.txt\n",
      "contentfile-19690114.html.txt\n",
      "contentfile-19700122.html.txt\n",
      "contentfile-19710122.html.txt\n",
      "contentfile-19720120.html.txt\n",
      "contentfile-19730202.html.txt\n",
      "contentfile-19740130.html.txt\n",
      "contentfile-19750115.html.txt\n",
      "contentfile-19760119.html.txt\n",
      "contentfile-19770112.html.txt\n",
      "contentfile-19780119.html.txt\n",
      "contentfile-19790125.html.txt\n",
      "contentfile-19800121.html.txt\n",
      "contentfile-19810116.html.txt\n",
      "contentfile-19820126.html.txt\n",
      "contentfile-19830125.html.txt\n",
      "contentfile-19840125.html.txt\n",
      "contentfile-19850206.html.txt\n",
      "contentfile-19860204.html.txt\n",
      "contentfile-19870127.html.txt\n",
      "contentfile-19880125.html.txt\n",
      "contentfile-19890209.html.txt\n",
      "contentfile-19900131.html.txt\n",
      "contentfile-19910129.html.txt\n",
      "contentfile-19920128.html.txt\n",
      "contentfile-19930217.html.txt\n",
      "contentfile-19940125.html.txt\n",
      "contentfile-19950124.html.txt\n",
      "contentfile-19960123.html.txt\n",
      "contentfile-19970204.html.txt\n",
      "contentfile-19980127.html.txt\n",
      "contentfile-19990119.html.txt\n",
      "contentfile-20000127.html.txt\n",
      "contentfile-20010227.html.txt\n",
      "contentfile-20010920.html.txt\n",
      "contentfile-20020129.html.txt\n",
      "contentfile-20030128.html.txt\n",
      "contentfile-20040120.html.txt\n",
      "contentfile-20050202.html.txt\n",
      "contentfile-20060131.html.txt\n",
      "contentfile-20070123.html.txt\n",
      "contentfile-20080128.html.txt\n",
      "contentfile-20090224.html.txt\n",
      "contentfile-20100127.html.txt\n",
      "contentfile-20110125.html.txt\n",
      "contentfile-20120124.html.txt\n",
      "contentfile-20130212.html.txt\n",
      "contentfile-20140128.html.txt\n",
      "contentfile-20150120.html.txt\n",
      "contentfile-20160112.html.txt\n",
      "contentfile-20170228.html.txt\n",
      "contentfile-20180130.html.txt\n",
      "contentfile-20190205.html.txt\n",
      "contentfile-20200204.html.txt\n",
      "contentfile-20210428.html.txt\n",
      "printing all the frequent pairs above value 10\n",
      "printing 20 frequent word-pairs\n",
      "[(('work', 'together'), 20), (('net', 'onetwothree'), 235), (('two', 'countries'), 12), (('united', 'america'), 17), (('god', 'america'), 19), (('subject', 'attention'), 12), (('every', 'us'), 11), (('right', 'thing'), 15), (('government', 'government'), 17), (('search', 'search'), 235), (('war', 'war'), 12), (('one', 'year'), 11), (('must', 'america'), 11), (('may', 'necessary'), 11), (('us', 'america'), 11), (('united', 'nations'), 11), (('bless', 'america'), 18), (('must', 'war'), 11), (('subject', 'congress'), 13), (('attention', 'congress'), 12)]\n"
     ]
    }
   ],
   "source": [
    "#This is the part 2 of the assignment.\n",
    "#in this part we calculate the frequency of co-occurence of words , Output 20 frequent pairs of words and lift between two words\n",
    "#and output those whose lift is bigger than 3.0\n",
    "import findspark\n",
    "findspark.init()\n",
    "import re\n",
    "from itertools import islice\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from collections import Counter, OrderedDict\n",
    "import urllib.request\n",
    "from statistics import stdev\n",
    "import re\n",
    "import urllib.request\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stopwords =stopwords.words('english')\n",
    "\n",
    "\n",
    "#here we are creating a new spark session\n",
    "spark = SparkSession.builder\\\n",
    "                    .master(\"local\")\\\n",
    "                    .appName('FirstAssignment1part2')\\\n",
    "                    .getOrCreate()\n",
    "sc=spark.sparkContext\n",
    "\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Safari/537.36',\n",
    "}\n",
    "\n",
    "\n",
    "#loading the website and its contents\n",
    "url = \"http://stateoftheunion.onetwothree.net/texts/index.html\"\n",
    "requ = urllib.request.Request(url,headers=headers)\n",
    "#To obtain the source code of the website\n",
    "req_url = urllib.request.urlopen(requ) \n",
    "contents = req_url.read()\n",
    "contents = str(contents)\n",
    "\n",
    "#writing the contents into alldata file.\n",
    "f = open('alldata.txt', 'w')\n",
    "f.writelines(contents)\n",
    "f.close()\n",
    "f = open('alldata.txt','r')\n",
    "contents = f.read()\n",
    "href_regex = r'href=[\\'\"]?([^\\'\" >]+)'\n",
    "pagenames = re.findall(href_regex, contents)\n",
    "allnames = list()\n",
    "\n",
    "#creating and saving the contents of each link into thier specific txt file such as contentfile-17901208.html.txt\n",
    "for index,pagename in enumerate(pagenames):\n",
    "    if pagename[0:2] == '17' or pagename[0:2] == '18' or pagename[0:2] == '19' or pagename[0:2] == '20':\n",
    "        allnames.append(pagename)\n",
    "        file = open('contentfile-%s.txt' % pagename, 'w')\n",
    "        url = \"http://stateoftheunion.onetwothree.net/texts/\"+pagename\n",
    "        req = urllib.request.Request(url,headers=headers)\n",
    "        #To obtain the source code of the website\n",
    "        request_url = urllib.request.urlopen(req) \n",
    "        contents = request_url.read()\n",
    "        file.write(str(contents))    \n",
    "\n",
    "#here we are removing the punctuation, html, or stopwords and removing them\n",
    "def RemoveNT(a):\n",
    "    a = a.replace(\"\\\\n\", \" \")\n",
    "    a = a.replace(\"\\\\t\", \" \")\n",
    "    return a\n",
    "\n",
    "def RemovePunc(a):\n",
    "    punc='!\"#$%&\\'()*+,./:;<=>?@[\\\\]^_`{|}~-'\n",
    "    lowcase = a.lower()\n",
    "    for ch in punc:\n",
    "        lowcase = lowcase.replace(ch, \" \")\n",
    "    return lowcase\n",
    "\n",
    "html_words = [\"href\",\"div\",\"index\",\"style\",\"tabs\",\"cell\",\"html\",\"link\",\n",
    "              \"src\",\"script\",\"image\",\"center\",\"alt\",\"html\",\"xml\",\"version\",\"meta\"\n",
    "              \"css\",\"http\",\"type\",\"script\",\"class\",\"text\",\"instance\",\"p\"]\n",
    "def RemoveHtml(a):\n",
    "    for word in html_words:\n",
    "        if a.count(word) > 0:\n",
    "            a = \" \"\n",
    "    return a\n",
    "        \n",
    "Tags = re.compile(r'<[^>]+>')\n",
    "def RemoveTags(a):\n",
    "    return Tags.sub(\"\", a)\n",
    "\n",
    "def RemoveStopw(a):\n",
    "    return \" \".join([word for word in str(a).split() if word not in stopwords])\n",
    "\n",
    "def RemoveDigit(a):\n",
    "    res = any(chr.isdigit() for chr in a)\n",
    "    if res:\n",
    "        a = \" \"\n",
    "    return a\n",
    "\n",
    "my_rdd = sc.emptyRDD()\n",
    "my_rdd2 = sc.emptyRDD()\n",
    "my_rdd_list = list()\n",
    "my_count_list = list()\n",
    "count = 0\n",
    "alllinksnew = allnames\n",
    "import itertools\n",
    "for index,pagename in enumerate(alllinksnew):\n",
    "    filename = \"contentfile-\"+pagename+\".txt\"\n",
    "    print(filename)\n",
    "    my_rdd=sc.textFile(filename)\n",
    "    my_rdd=my_rdd.flatMap(lambda satir: satir.split(\".\"))\n",
    "    my_rdd2 = my_rdd.map(RemoveNT)\n",
    "    my_rdd2 = my_rdd2.map(RemovePunc)\n",
    "    my_rdd2 = my_rdd2.map(RemoveHtml)\n",
    "    my_rdd2 = my_rdd2.map(RemoveTags)\n",
    "    my_rdd2 = my_rdd2.map(RemoveStopw)\n",
    "    my_rdd2 = my_rdd2.map(RemoveDigit)\n",
    "    my_rdd2 = my_rdd2.filter(lambda x:x!='')\n",
    "    my_rdd2 = my_rdd2.filter(lambda x:x!=' ')\n",
    "    my_rdd_list.append(my_rdd2)\n",
    "\n",
    "rdd = sc.union(my_rdd_list)\n",
    "final = (rdd.filter(lambda x: x != \"\").map(lambda x: x.split(\" \"))\n",
    "        .flatMap(lambda x: itertools.combinations(x, 2))\n",
    "        .filter(lambda x: x[0] != \"\")\n",
    "        .map(lambda x: (x, 1))\n",
    "        .reduceByKey(lambda x, y: x + y)\n",
    "        )\n",
    "final1=final.collect()\n",
    "\n",
    "final=final.filter(lambda KeyValue: KeyValue[1]>10)\n",
    "\n",
    "#here we are printing all the frequent pairs above value 10\n",
    "print(\"printing all the frequent pairs above value 10\")\n",
    "print(final.collect())\n",
    "\n",
    "#here we are printing 20 frequent pairs of words\n",
    "print(\"printing 20 frequent word-pairs\")\n",
    "print(final.take(20))\n",
    "rdd2 = rdd.flatMap(lambda satir: satir.split(\" \"))\n",
    "rdd2 = rdd2.map(lambda  word:(word,1))\n",
    "rdd2 = rdd2.reduceByKey(lambda x,y:(x+y)).sortByKey()\n",
    "my_count_list=rdd2.collect()\n",
    "\n",
    "#calculating the lift of the pairs greater than 3\n",
    "lift=0\n",
    "for m in final1:\n",
    "    i=j=0\n",
    "    for n in my_count_list:\n",
    "        if m[0][0] == n[0]:\n",
    "            i= n[1]\n",
    "        if m[0][1] == n[0]:\n",
    "            j= n[1]\n",
    "            j=j/100\n",
    "    if(i!=0 and j!=0):\n",
    "        lift=(m[1]/(i*j))\n",
    "    if(lift>3):\n",
    "        print(\"the lift greater than 3 for the word\",m[0], \"is\", lift)\n",
    "\n",
    "\n",
    "# To stop the current spark session\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e55ef9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
